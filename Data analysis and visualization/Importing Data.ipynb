{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Objects - Reading and Writing to Files\n",
    "https://www.youtube.com/watch?v=Uh2ebFW8OYM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C is OS\n",
      " Volume Serial Number is 0686-879D\n",
      "\n",
      " Directory of C:\\Users\\Xueho\\Projects\\Py_Jupyter_Proj\\Raw_data\n",
      "\n",
      "01/30/2020  11:50 AM             1,817 names.csv\n",
      "               1 File(s)          1,817 bytes\n",
      "               0 Dir(s)  136,661,647,360 bytes free\n"
     ]
    }
   ],
   "source": [
    "ls name*.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DirEntry', 'F_OK', 'MutableMapping', 'O_APPEND', 'O_BINARY', 'O_CREAT', 'O_EXCL', 'O_NOINHERIT', 'O_RANDOM', 'O_RDONLY', 'O_RDWR', 'O_SEQUENTIAL', 'O_SHORT_LIVED', 'O_TEMPORARY', 'O_TEXT', 'O_TRUNC', 'O_WRONLY', 'P_DETACH', 'P_NOWAIT', 'P_NOWAITO', 'P_OVERLAY', 'P_WAIT', 'PathLike', 'R_OK', 'SEEK_CUR', 'SEEK_END', 'SEEK_SET', 'TMP_MAX', 'W_OK', 'X_OK', '_Environ', '__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_execvpe', '_exists', '_exit', '_fspath', '_get_exports_list', '_putenv', '_unsetenv', '_wrap_close', 'abc', 'abort', 'access', 'altsep', 'chdir', 'chmod', 'close', 'closerange', 'cpu_count', 'curdir', 'defpath', 'device_encoding', 'devnull', 'dup', 'dup2', 'environ', 'error', 'execl', 'execle', 'execlp', 'execlpe', 'execv', 'execve', 'execvp', 'execvpe', 'extsep', 'fdopen', 'fsdecode', 'fsencode', 'fspath', 'fstat', 'fsync', 'ftruncate', 'get_exec_path', 'get_handle_inheritable', 'get_inheritable', 'get_terminal_size', 'getcwd', 'getcwdb', 'getenv', 'getlogin', 'getpid', 'getppid', 'isatty', 'kill', 'linesep', 'link', 'listdir', 'lseek', 'lstat', 'makedirs', 'mkdir', 'name', 'open', 'pardir', 'path', 'pathsep', 'pipe', 'popen', 'putenv', 'read', 'readlink', 'remove', 'removedirs', 'rename', 'renames', 'replace', 'rmdir', 'scandir', 'sep', 'set_handle_inheritable', 'set_inheritable', 'spawnl', 'spawnle', 'spawnv', 'spawnve', 'st', 'startfile', 'stat', 'stat_result', 'statvfs_result', 'strerror', 'supports_bytes_environ', 'supports_dir_fd', 'supports_effective_ids', 'supports_fd', 'supports_follow_symlinks', 'symlink', 'sys', 'system', 'terminal_size', 'times', 'times_result', 'truncate', 'umask', 'uname_result', 'unlink', 'urandom', 'utime', 'waitpid', 'walk', 'write']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(dir(os))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xueho\\Projects\\Python_projects\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:\\xx\\xx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir()\n",
    "os.mkdir(''), \n",
    "os.makedirs('') # also create intermediate directories\n",
    "os.rmdir('')\n",
    "os.removedirs('') # also remove intermediate directories\n",
    "os.rename('old', 'new')\n",
    "os.stat('xxx.xxx').st_size \n",
    "os.stat('xxx.xxx').st_mtime # last modified time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.walk(''): print out al lthe directories and fles within them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Xueho\\\\Projects\\\\'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ.get('HOME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.join(os.environ.get('HOME'), 'xx.xxx') # create file under the directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.exists('typek') # path.isdir, path.isfile, .path.splitext (split root from extension) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(os.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "File Objects\n",
    "#The Basics:\n",
    "f = open(\"test.txt\", \"r\")\n",
    "f = open(\"test.txt\", \"w\")\n",
    "f = open(\"test.txt\", \"a\")\n",
    "f = open(\"test.txt\", \"r+\")\n",
    "print(f.name)\n",
    "print(f.mode)\n",
    "f.close()\n",
    "\n",
    "#Reading Files:\n",
    "with open(\"test.txt\", \"r\") as f:\n",
    "    # Small Files:\n",
    "    f_contents = f.read()\n",
    "    print(f_contents)\n",
    " \n",
    "    #Big Files:\n",
    "    f_contents = f.readlines()\n",
    "    print(f_contents)\n",
    "  \n",
    "    # With the extra lines:\n",
    "    f_contents = f.readline()\n",
    "    print(f_contents)\n",
    "    f_contents = f.readline()\n",
    "    print(f_contents)\n",
    " \n",
    "    # Without the extra lines:\n",
    "    f_contents = f.readline()\n",
    "    print(f_contents, end = '')\n",
    "    f_contents = f.readline()\n",
    "    print(f_contents, end = '')\n",
    " \n",
    "    #Iterating through the file:\n",
    "    for line in f:\n",
    "        print(line, end = '')\n",
    " \n",
    "    # Going Back....:\n",
    "    f_contents = f.read()\n",
    "    print(f_contents, end = '')\n",
    " \n",
    "    # Printing by characters:\n",
    "    f_contents = f.read(100)\n",
    "    print(f_contents, end = '')\n",
    "    f_contents = f.read(100)\n",
    "    print(f_contents, end = '')\n",
    "    f_contents = f.read(100)\n",
    "    print(f_contents, end = '')\n",
    "\n",
    "   #I terating through small chunks:\n",
    "    size_to_read = 100\n",
    "    f_contents = f.read(size_to_read)\n",
    "    while len(f_contents) > 0:\n",
    "        print(f_contents)\n",
    "        f_contents = f.read(size_to_read)\n",
    "\n",
    "   # Iterating through small chunks, with 10 characters:\n",
    "    size_to_read = 10\n",
    "    f_contents = f.read(size_to_read)\n",
    "    print(f_contents, end = '')\n",
    "    f.seek(0) # set position back to the begining of the file\n",
    "    f_contents = f.read(size_to_read)\n",
    "    print(f_contents, end = '')\n",
    "    print(f.tell()) # current position in the file\n",
    "    while len(f_contents) > 0:\n",
    "        print(f_contents, end = '*')\n",
    "        f_contents = f.read(size_to_read)\n",
    "print(f.mode)\n",
    "print(f.closed)\n",
    "print(f.read())\n",
    "\n",
    "# Writing Files:\n",
    "# The Error:\n",
    "with open(\"test.txt\", \"r\") as f:\n",
    "    f.write(\"Test\")\n",
    "\n",
    "# Writing Starts:\n",
    "with open(\"test2.txt\", \"w\") as f:\n",
    "    f.write(\"Test\")\n",
    "    f.seek(0)\n",
    "    f.write(\"Test\")\n",
    "    f.seek(\"R\") # R will only overwrite \"T\"\n",
    "\n",
    "# Copying Files:\n",
    "with open(\"test.txt\", \"r\") as rf:\n",
    "    with open(\"test_copy.txt\", \"w\") as wf:\n",
    "        for line in rf:\n",
    "            wf.write(line)\n",
    "\n",
    "Copying image:\n",
    "# # The Error\n",
    "# with open(\"bronx.jpg\", \"r\") as rf:\n",
    "#     with open(\"bronx_copy.jpg\", \"w\") as wf:\n",
    "#         for line in rf:\n",
    "#             wf.write(line)\n",
    "# \n",
    "# Copying the image starts, without chunks:\n",
    "\n",
    "with open(\"bronx.jpg\", \"rb\") as rf: # need to use binary mode to work with pictures\n",
    "    with open(\"bronx_copy.jpg\", \"wb\") as wf:\n",
    "        for line in rf:\n",
    "            wf.write(line)\n",
    "\n",
    "# Copying the image with chunks:\n",
    "with open(\"bronx.jpg\", \"rb\") as rf:\n",
    "    with open(\"bronx_copy.jpg\", \"wb\") as wf:\n",
    "        chunk_size = 4096\n",
    "        rf_chunk = rf.read(chunk_size)\n",
    "        while len(rf_chunk) > 0:\n",
    "            wf.write(rf_chunk)\n",
    "            rf_chunk = rf.read(chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir('/path/to/files/')\n",
    "\n",
    "# Am I in the correct directory?\n",
    "print(os.getcwd())\n",
    "print(dir(os))\n",
    "\n",
    "# Print all the current file names\n",
    "for f in os.listdir():\n",
    "    # If .DS_Store file is created, ignore it\n",
    "    if f == '.DS_Store':\n",
    "        continue\n",
    "    file_name, file_ext = os.path.splitext(f)\n",
    "    print(file_name)\n",
    "\n",
    "    # One way to do this\n",
    "    f_title, f_course, f_number = file_name.split('-')\n",
    "\n",
    "    print('{}-{}-{}{}'.format(f_number, f_course, f_title, file_ext))\n",
    "\n",
    "    # Need to remove whitespace\n",
    "    f_title = f_title.strip()\n",
    "    f_course = f_course.strip()\n",
    "    f_number = f_number.strip()\n",
    "\n",
    "    # Want to remove the number sign?\n",
    "    f_number = f_number.strip()[1:]\n",
    "\n",
    "    # Instead of 1, make it 01 using zfill\n",
    "    f_number = f_number.strip()[1:].zfill(2)\n",
    "\n",
    "    print('{}-{}-{}{}'.format(f_number, f_course, f_title, file_ext))\n",
    "\n",
    "    # reformat\n",
    "    print('{}-{}{}'.format(f_number, f_title.strip(), file_ext.strip()))\n",
    "\n",
    "    new_name = '{}-{}{}'.format(file_num, file_title, file_ext)\n",
    "\n",
    "    os.rename(fn, new_name)\n",
    "\n",
    "print(len(os.listdir()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p>There are currently 30 public contributors. Thank You!</p>\n",
      "<ul>\n",
      "\t<li>John Doe</li>\n",
      "\t<li>Dave Smith</li>\n",
      "\t<li>Mary Jacobs</li>\n",
      "\t<li>Jane Stuart</li>\n",
      "\t<li>Tom Wright</li>\n",
      "\t<li>Steve Robinson</li>\n",
      "\t<li>Nicole Jacobs</li>\n",
      "\t<li>Jane Wright</li>\n",
      "\t<li>Jane Doe</li>\n",
      "\t<li>Kurt Wright</li>\n",
      "\t<li>Kurt Robinson</li>\n",
      "\t<li>Jane Jenkins</li>\n",
      "\t<li>Neil Robinson</li>\n",
      "\t<li>Tom Patterson</li>\n",
      "\t<li>Sam Jenkins</li>\n",
      "\t<li>Steve Stuart</li>\n",
      "\t<li>Maggie Patterson</li>\n",
      "\t<li>Maggie Stuart</li>\n",
      "\t<li>Jane Doe</li>\n",
      "\t<li>Steve Patterson</li>\n",
      "\t<li>Dave Smith</li>\n",
      "\t<li>Sam Wilks</li>\n",
      "\t<li>Kurt Jefferson</li>\n",
      "\t<li>Sam Stuart</li>\n",
      "\t<li>Jane Stuart</li>\n",
      "\t<li>Dave Davis</li>\n",
      "\t<li>Sam Patterson</li>\n",
      "\t<li>Tom Jefferson</li>\n",
      "\t<li>Jane Stuart</li>\n",
      "\t<li>Maggie Jefferson</li>\n",
      "</ul>\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "html_output = ''\n",
    "names = []\n",
    "\n",
    "with open('Raw_data\\patrons.csv', 'r') as data_file:\n",
    "    csv_data = csv.DictReader(data_file)\n",
    "\n",
    "    # don't want first line of bad data\n",
    "    next(csv_data)\n",
    "\n",
    "    for line in csv_data:\n",
    "        if line['FirstName'] == 'No Reward':\n",
    "            break\n",
    "        names.append(f\"{line['FirstName']} {line['LastName']}\")\n",
    "\n",
    "html_output += f'<p>There are currently {len(names)} public contributors. Thank You!</p>'\n",
    "\n",
    "html_output += '\\n<ul>'\n",
    "\n",
    "for name in names:\n",
    "    html_output += f'\\n\\t<li>{name}</li>'\n",
    "\n",
    "html_output += '\\n</ul>'\n",
    "\n",
    "print(html_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "f = open('..\\Raw_data\\guns.csv')\n",
    "data = list(csv.reader(f))\n",
    "header = data[0]\n",
    "data = data[1:]\n",
    "data[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "f=open('nfl_suspensions_data.csv')\n",
    "nfl_suspensions=list(csv.reader(f))\n",
    "nfl_suspensions=nfl_suspensions[1:]\n",
    "years={}\n",
    "for row in nfl_suspensions:\n",
    "    row_year=row[5]\n",
    "    if row_year in years:\n",
    "        years[row_year] +=1\n",
    "    else:\n",
    "        years[row_year] = 1\n",
    "print(years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<built-in method tell of _io.TextIOWrapper object at 0x0000018055FDE828>\n"
     ]
    }
   ],
   "source": [
    "with open('names.csv', 'r') as f:\n",
    "    print(f.tell)\n",
    "    f.mode ('r', 'w', 'a', 'r+(r&w)')\n",
    "    f.read(100)  # read first 100 lines \n",
    "    f.tell() \n",
    "    f.seek(0) # set positio back to the begining of the file\n",
    "\n",
    "with open('test.csv', 'r') as rf:\n",
    "    with open('test_copy.csv', 'w') as wf:\n",
    "        for line in rf:\n",
    "            wf.write(line)\n",
    "            \n",
    "with open('dog.jpg', 'rb') as rf:\n",
    "    with open('dog_copy.jpg', 'wb') as wf:\n",
    "        for line in rf:\n",
    "            wf.write(line)\n",
    "            \n",
    "with open('dog.jpg', 'rb') as rf:\n",
    "    with open('dog_copy.jpg', 'wb') as wf:\n",
    "        chunk_size = 4096\n",
    "        rf_chunk = rf.read(chunk_size)\n",
    "        while len(rf_chunk) > 0:\n",
    "            wf.write(rf_chunk)\n",
    "            rf_chunk = rf.read(chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy\n",
    "import re\n",
    "\n",
    "data_files = [\n",
    "    \"ap_2010.csv\",\n",
    "    \"class_size.csv\",\n",
    "    \"demographics.csv\",\n",
    "    \"graduation.csv\",\n",
    "    \"hs_directory.csv\",\n",
    "    \"sat_results.csv\"\n",
    "]\n",
    "\n",
    "data = {}\n",
    "\n",
    "for f in data_files:\n",
    "    d = pd.read_csv(\"schools/{0}\".format(f))\n",
    "    data[f.replace(\".csv\", \"\")] = d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV Module - How to Read, Parse, and Write CSV Files\n",
    "https://www.youtube.com/watch?v=q5uM4VKywbA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.youtube.com/watch?v=q5uM4VKywbA\n",
    "import csv\n",
    "\n",
    "with open('Raw_data\\names.csv', 'r') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file) # delimiter=',' is defalut value\n",
    "    # next(csv_reader) # skip first line of headers\n",
    "\n",
    "    with open('Raw_data\\new_names.csv', 'w') as new_file:\n",
    "        csv_writer = csv.writer(new_file, delimiter='-')\n",
    "        \n",
    "        for line in csv_reader:\n",
    "            del line[2]\n",
    "            csv_writer.writerow(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('new_names.csv', 'w') as new_file:  # generate error when including Raw_data\\\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Raw_data\\names.csv', 'r') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file) # field names becomes key for dict\n",
    "       \n",
    "    with open('Raw_data\\new_names.csv', 'w') as new_file:\n",
    "        fieldnames = ['first_name', 'last_name']\n",
    "        csv_writer = csv.DictWriter(new_file, fieldnames = filednames, delimiter='\\t')\n",
    "        # csv_writer = csv.DictWriter(new_file, fieldnames=fieldnames, delimiter='\\t')\n",
    "        csv_writer.writeheader()\n",
    "        \n",
    "        for line in csv_reader:\n",
    "            del line['email']\n",
    "            csv_writer.writerow(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the ownership of the ~/.local/share/jupyter directory from root to user\n",
    "sudo chown -R sura:sura /home/sura/.local/share/jupyter  #didn't work\n",
    "sudo chown -R user:user ~/.local/share/jupyter #didn't work\n",
    "sudo jupyter notebook --allow-root  #didn't work\n",
    "\n",
    "# Change folder(.local) permissions by this command:\n",
    "sudo chmod -R 777 .local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing Names From a CSV to an HTML List\n",
    "https://www.youtube.com/watch?v=bkpLhQd6YQM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing Names From a CSV to an HTML List: https://www.youtube.com/watch?v=bkpLhQd6YQM\n",
    "import csv\n",
    "html_output = ''\n",
    "names = []\n",
    "with open('Raw_data\\patrons.csv', 'r') as data_file:\n",
    "    csv_data = csv.DictReader(data_file)\n",
    "    # skip first line of bad data\n",
    "    next(csv_data)\n",
    "    for line in csv_data:\n",
    "        if line['FirstName'] == 'No Reward':\n",
    "            break\n",
    "        names.append(f\"{line['FirstName']} {line['LastName']}\")\n",
    "\n",
    "html_output += f'<p>There are currently {len(names)} public contributors. Thank You!</p>'\n",
    "html_output += '\\n<ul>'\n",
    "\n",
    "for name in names:\n",
    "    html_output += f'\\n\\t<li>{name}</li>'\n",
    "html_output += '\\n</ul>'\n",
    "print(html_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automate Parsing and Renaming of Multiple Files\n",
    "https://www.youtube.com/watch?v=ve2pmm5JqmI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir('/path/to/files/') # drag the target folder to termminal to tuaocomplete path\n",
    "\n",
    "# Am I in the correct directory?\n",
    "# print(os.getcwd())\n",
    "# print(dir(os))\n",
    "# Print all the current file names\n",
    "for f in os.listdir():\n",
    "    # print(f)\n",
    "    # print(os.path.splitext(f))\n",
    "    # If .DS_Store file is created, ignore it\n",
    "    if f == '.DS_Store':\n",
    "        continue\n",
    "\n",
    "    f_name, f_ext = os.path.splitext(f)\n",
    "    # print(f_name)\n",
    "    # print(f_name.split('-'))\n",
    "\n",
    "    # One way to do this\n",
    "    f_title, f_course, f_number = f_name.split('-')\n",
    "\n",
    "    # print('{}-{}-{}{}'.format(f_number, f_course, f_title, file_ext))\n",
    "\n",
    "    # Need to remove whitespace\n",
    "    f_title = f_title.strip()\n",
    "    f_course = f_course.strip()\n",
    "    # f_number = f_number.strip()\n",
    "\n",
    "    # Want to remove the number sign?\n",
    "    # f_number = f_number.strip()[1:]\n",
    "\n",
    "    # One thing I noticed about this output is that if it was sorted by filename\n",
    "    # then the 1 and 10 would be next to each other. How do we fix this? One way we can fix this is to pad\n",
    "    # the numbers. So instead of 1, we'll make it 01. If we had hundreds of files then this would maybe need to be 001.\n",
    "    # We can do this in Python with zfill\n",
    "    f_number = f_number.strip()[1:].zfill(2)\n",
    "\n",
    "    # print('{}-{}-{}{}'.format(f_number, f_course, f_title, file_ext))\n",
    "\n",
    "    # You have the power to reformat in any way you see fit\n",
    "    print('{}-{}{}'.format(f_number, f_title.strip(), file_ext.strip()))\n",
    "\n",
    "    new_name = '{}-{}{}'.format(f_number, f_title, f_ext)\n",
    "\n",
    "    os.rename(fn, new_name)\n",
    "\n",
    "\n",
    "# print(len(os.listdir()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open a file: file\n",
    "file = open('moby_dick.txt', 'r')\n",
    "\n",
    "# Print it\n",
    "print(file.read())\n",
    "\n",
    "# Check whether file is closed\n",
    "print(file.closed)\n",
    "\n",
    "# Close file\n",
    "file.close()\n",
    "\n",
    "# Check whether file is closed\n",
    "print(file.closed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read & print the first 3 lines\n",
    "with open('moby_dick.txt') as file:\n",
    "    print(file.readline())\n",
    "    print(file.readline())\n",
    "    print(file.readline())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import numpy\n",
    "import numpy as np\n",
    "\n",
    "# Assign the filename: file\n",
    "file = 'digits_header.txt'\n",
    "\n",
    "# Load the data: data\n",
    "data = np.loadtxt(file, delimiter='\\t', skiprows=1, usecols=[0,2])\n",
    "\n",
    "# Print data\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the filename: file\n",
    "file = 'titanic.csv'\n",
    "\n",
    "# Import file using np.recfromcsv: d\n",
    "d = np.recfromcsv('titanic.csv')\n",
    "\n",
    "# Print out first three entries of d\n",
    "print(d[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pickle package\n",
    "import pickle\n",
    "\n",
    "# Open pickle file and load data: d\n",
    "with open('data.pkl', 'rb') as file:\n",
    "    d = pickle.load(file)\n",
    "\n",
    "# Print d\n",
    "print(d)\n",
    "\n",
    "# Print datatype of d\n",
    "print(type(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Assign spreadsheet filename: file\n",
    "file = 'battledeath.xlsx'\n",
    "\n",
    "# Load spreadsheet: xl\n",
    "xl = pd.ExcelFile(file)\n",
    "\n",
    "# Print sheet names\n",
    "print(xl.sheet_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sheet into a DataFrame by name: df1\n",
    "df1 = xl.parse('2004')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the first sheet and rename the columns: df1\n",
    "df1 = xl.parse(0, skiprows=[0], names=['Country','AAM due to War (2002)'])\n",
    "\n",
    "# Print the head of the DataFrame df1\n",
    "print(df1.head())\n",
    "\n",
    "# Parse the first column of the second sheet and rename the column: df2\n",
    "df2 = xl.parse(1, parse_cols=[0], skiprows=[0], names=['Country'])\n",
    "\n",
    "# Print the head of the DataFrame df2\n",
    "print('df2\\n')\n",
    "print(df2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Import necessary module\n",
    "from sqlalchemy import create_engine \n",
    "\n",
    "# Create engine: engine\n",
    "engine = create_engine('sqlite:///Chinook.sqlite')\n",
    "\n",
    "# Save the table names to a list: table_names\n",
    "table_names = engine.table_names()\n",
    "\n",
    "# Print the table names to the shell\n",
    "print(table_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "# Create engine: engine\n",
    "engine = create_engine('sqlite:///Chinook.sqlite')\n",
    "\n",
    "# Open engine connection: con\n",
    "con = engine.connect()\n",
    "\n",
    "# Perform query: rs\n",
    "rs = con.execute(\"SELECT * FROM Album\")\n",
    "\n",
    "# Save results of the query to DataFrame: df\n",
    "df = pd.DataFrame(rs.fetchall())\n",
    "\n",
    "# Close connection\n",
    "con.close()\n",
    "\n",
    "# Print head of DataFrame df\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open engine in context manager\n",
    "# Perform query and save results to DataFrame: df\n",
    "with engine.connect() as con:\n",
    "    rs = con.execute(\"SELECT LastName,Title FROM Employee\")\n",
    "    df = pd.DataFrame(rs.fetchmany(size=3))\n",
    "    df.columns = rs.keys()\n",
    "\n",
    "# Print the length of the DataFrame df\n",
    "print(len(df))\n",
    "\n",
    "# Print the head of the DataFrame df\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create engine: engine\n",
    "engine = create_engine('sqlite:///Chinook.sqlite')\n",
    "\n",
    "# Open engine in context manager\n",
    "with engine.connect() as con:\n",
    "    rs = con.execute(\"SELECT * FROM Employee ORDER BY BirthDate\")\n",
    "    df = pd.DataFrame(rs.fetchall())\n",
    "\n",
    "    # Set the DataFrame's column names\n",
    "df.columns =rs.keys()\n",
    "\n",
    "# Print head of DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "# Create engine: engine\n",
    "engine = create_engine('sqlite:///Chinook.sqlite')\n",
    "\n",
    "# Execute query and store records in DataFrame: df\n",
    "df = pd.read_sql_query(\"SELECT * FROM Album\", engine)\n",
    "\n",
    "# Print head of DataFrame\n",
    "print(df.head())\n",
    "\n",
    "# Open engine in context manager and store query result in df1\n",
    "with engine.connect() as con:\n",
    "    rs = con.execute(\"SELECT * FROM Album\")\n",
    "    df1 = pd.DataFrame(rs.fetchall())\n",
    "    df1.columns = rs.keys()\n",
    "\n",
    "# Confirm that both methods yield the same result\n",
    "print(df.equals(df1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import package\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Assign url of file: url\n",
    "url = 'https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/datasets/winequality-red.csv'\n",
    "\n",
    "# Save file locally\n",
    "urlretrieve(url, 'winequality-red.csv')\n",
    "\n",
    "# Read file into a DataFrame and print its head\n",
    "df = pd.read_csv('winequality-red.csv', sep=';')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Assign url of file: url\n",
    "url = 'https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/datasets/winequality-red.csv'\n",
    "\n",
    "# Read file into a DataFrame: df\n",
    "df = pd.read_csv(url, sep = ';')\n",
    "\n",
    "# Print the head of the DataFrame\n",
    "print(df.head())\n",
    "\n",
    "# Plot first column of df\n",
    "pd.DataFrame.hist(df.ix[:, 0:1])\n",
    "plt.xlabel('fixed acidity (g(tartaric acid)/dm$^3$)')\n",
    "plt.ylabel('count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import package\n",
    "import pandas as pd\n",
    "\n",
    "# Assign url of file: url\n",
    "url = 'http://s3.amazonaws.com/assets.datacamp.com/course/importing_data_into_r/latitude.xls'\n",
    "\n",
    "# Read in all sheets of Excel file: xl\n",
    "xl = pd.read_excel(url, sheetname = None) \n",
    "\n",
    "# Print the sheetnames to the shell\n",
    "print(xl.keys())\n",
    "\n",
    "# Print the head of the first sheet (using its name, NOT its index)\n",
    "print(xl['1700'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'http.client.HTTPResponse'>\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "from urllib.request import urlopen, Request \n",
    "\n",
    "# Specify the url\n",
    "url = \"http://www.datacamp.com/teach/documentation\"\n",
    "\n",
    "# This packages the request: request\n",
    "request = Request(url)\n",
    "\n",
    "# Sends the request and catches the response: response\n",
    "response = urlopen(request)\n",
    "\n",
    "# Print the datatype of response\n",
    "print(type(response))\n",
    "\n",
    "# Be polite and close the response!\n",
    "response.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "from urllib.request import urlopen, Request\n",
    "\n",
    "# Specify the url\n",
    "url = \"http://www.datacamp.com/teach/documentation\"\n",
    "\n",
    "# This packages the request\n",
    "request = Request(url)\n",
    "\n",
    "# Sends the request and catches the response: response\n",
    "response = urlopen(request)\n",
    "\n",
    "# Extract the response: html\n",
    "html = response.read()\n",
    "\n",
    "# Print the html\n",
    "print(html)\n",
    "\n",
    "# Be polite and close the response!\n",
    "response.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import package\n",
    "import requests\n",
    "\n",
    "# Specify the url: url\n",
    "url = \"http://www.datacamp.com/teach/documentation\"\n",
    "\n",
    "# Packages the request, send the request and catch the response: r\n",
    "r = requests.get(url)\n",
    "\n",
    "# Extract the response: text\n",
    "text = r.text\n",
    "\n",
    "# Print the html\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Specify url: url\n",
    "url = 'https://www.python.org/~guido/'\n",
    "\n",
    "# Package the request, send the request and catch the response: r\n",
    "r = requests.get(url)\n",
    "\n",
    "# Extracts the response as html: html_doc\n",
    "html_doc = r.text\n",
    "\n",
    "# Create a BeautifulSoup object from the HTML: soup\n",
    "soup = BeautifulSoup(html_doc)\n",
    "\n",
    "# Prettify the BeautifulSoup object: pretty_soup\n",
    "pretty_soup = soup.prettify()\n",
    "\n",
    "# Print the response\n",
    "print(pretty_soup)\n",
    "\n",
    "# Get the title of Guido's webpage: guido_title\n",
    "guido_title = soup.title\n",
    "\n",
    "# Print the title of Guido's webpage to the shell\n",
    "print(guido_title)\n",
    "\n",
    "# Get Guido's text: guido_text\n",
    "guido_text = soup.get_text()\n",
    "\n",
    "# Print Guido's text to the shell\n",
    "print(guido_text)\n",
    "\n",
    "# Find all 'a' tags (which define hyperlinks): a_tags\n",
    "a_tags=soup.find_all('a')\n",
    "\n",
    "# Print the URLs to the shell\n",
    "for link in a_tags:\n",
    "    print(link.get('href'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigating airplane accident"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project aims to analyze a data set containing 77,282 aviation accidents occurred in the U.S. and the metadata associated with them.  The data are obtained from data.gov."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event Id | Investigation Type | Accident Number | Event Date | Location | Country | Latitude | Longitude | Airport Code | Airport Name | Injury Severity | Aircraft Damage | Aircraft Category | Registration Number | Make | Model | Amateur Built | Number of Engines | Engine Type | FAR Description | Schedule | Purpose of Flight | Air Carrier | Total Fatal Injuries | Total Serious Injuries | Total Minor Injuries | Total Uninjured | Weather Condition | Broad Phase of Flight | Report Status | Publication Date | \n",
      "\n",
      "[['20001218X45447', 'Accident', 'LAX94LA336', '07/19/1962', 'BRIDGEPORT, CA', 'United States', '', '', '', '', 'Fatal(4)', 'Destroyed', '', 'N5069P', 'PIPER', 'PA24-180', 'No', '1', 'Reciprocating', '', '', 'Personal', '', '4', '0', '0', '0', 'UNK', 'UNKNOWN', 'Probable Cause', '09/19/1996', '\\n']]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "f = open(\"../Raw_data/AviationData.txt\")\n",
    "aviation_data = f.readlines()\n",
    "print(aviation_data[0])\n",
    "aviation_list = []\n",
    "for line in aviation_data:\n",
    "    aviation_list.append(line.split(\" | \"))\n",
    "lax_code = []\n",
    "for line in aviation_list:\n",
    "    if 'LAX94LA336' in line:\n",
    "        lax_code.append(line)\n",
    "print(lax_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['20001218X45447', 'Accident', 'LAX94LA336', '07/19/1962', 'BRIDGEPORT, CA', 'United States', '', '', '', '', 'Fatal(4)', 'Destroyed', '', 'N5069P', 'PIPER', 'PA24-180', 'No', '1', 'Reciprocating', '', '', 'Personal', '', '4', '0', '0', '0', 'UNK', 'UNKNOWN', 'Probable Cause', '09/19/1996', '\\n']]\n"
     ]
    }
   ],
   "source": [
    "# simplify algorithm using only one loop\n",
    "import csv\n",
    "f = open(\"../Raw_data/AviationData.txt\")\n",
    "aviation_data = f.readlines()\n",
    "aviation_list = []\n",
    "lax_code = []\n",
    "for line in aviation_data:\n",
    "    line = line.split(\" | \")\n",
    "    aviation_list.append(line)\n",
    "    if 'LAX94LA336' == line[2]:\n",
    "        lax_code.append(line)\n",
    "print(lax_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['20001218X45447', 'Accident', 'LAX94LA336', '07/19/1962', 'BRIDGEPORT, CA', 'United States', '', '', '', '', 'Fatal(4)', 'Destroyed', '', 'N5069P', 'PIPER', 'PA24-180', 'No', '1', 'Reciprocating', '', '', 'Personal', '', '4', '0', '0', '0', 'UNK', 'UNKNOWN', 'Probable Cause', '09/19/1996', '\\n']]\n"
     ]
    }
   ],
   "source": [
    "# simplify algorithm using list comprehension\n",
    "import csv\n",
    "f = open(\"..\\Raw_data\\AviationData.txt\")\n",
    "aviation_data = f.readlines()\n",
    "aviation_list = [s.split(\" | \") for s in aviation_data]\n",
    "lax_code = [s for s in aviation_list if 'LAX94LA336' == s[2]]\n",
    "print(lax_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['20001218X45447', 'Accident', 'LAX94LA336', '07/19/1962', 'BRIDGEPORT, CA', 'United States', '', '', '', '', 'Fatal(4)', 'Destroyed', '', 'N5069P', 'PIPER', 'PA24-180', 'No', '1', 'Reciprocating', '', '', 'Personal', '', '4', '0', '0', '0', 'UNK', 'UNKNOWN', 'Probable Cause', '09/19/1996', '\\n']]\n"
     ]
    }
   ],
   "source": [
    "# using dictionary\n",
    "import csv\n",
    "f = open(\"../Raw_data/AviationData.txt\")\n",
    "aviation_data = f.readlines()\n",
    "aviation_list = [s.split(\" | \") for s in aviation_data]\n",
    "aviation_dict_list = []\n",
    "key_ls = aviation_list[0]\n",
    "lax_code = []\n",
    "for line in aviation_list[1:]:\n",
    "    dict = {k: v for k, v in zip(key_ls, line)}\n",
    "    aviation_dict_list.append(dict)\n",
    "    if 'LAX94LA336' in dict.values():\n",
    "        lax_code.append(line)\n",
    "print(lax_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count how many accidents occurred in each U.S. state? which state had the most accidents overall?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Event Id', 'Investigation Type', 'Accident Number', 'Event Date', 'Location', 'Country', 'Latitude', 'Longitude', 'Airport Code', 'Airport Name', 'Injury Severity', 'Aircraft Damage', 'Aircraft Category', 'Registration Number', 'Make', 'Model', 'Amateur Built', 'Number of Engines', 'Engine Type', 'FAR Description', 'Schedule', 'Purpose of Flight', 'Air Carrier', 'Total Fatal Injuries', 'Total Serious Injuries', 'Total Minor Injuries', 'Total Uninjured', 'Weather Condition', 'Broad Phase of Flight', 'Report Status', 'Publication Date', '\\n'], ['20190712X54743', 'Accident', 'GAA19CA408', '07/12/2019', 'Hamilton, MT', 'United States', '46.251389', '-114.125556', '6S5', 'Ravalli County', 'Unavailable', 'Substantial', 'Airplane', 'N7409K', 'Piper', 'PA 20', 'No', '1', '', 'Part 91: General Aviation', '', 'Personal', '', '', '', '', '', 'VMC', '', 'Preliminary', '07/12/2019', '\\n']] \n",
      "\n",
      "[{'Event Id': '20190712X54743', 'Investigation Type': 'Accident', 'Accident Number': 'GAA19CA408', 'Event Date': '07/12/2019', 'Location': 'Hamilton, MT', 'Country': 'United States', 'Latitude': '46.251389', 'Longitude': '-114.125556', 'Airport Code': '6S5', 'Airport Name': 'Ravalli County', 'Injury Severity': 'Unavailable', 'Aircraft Damage': 'Substantial', 'Aircraft Category': 'Airplane', 'Registration Number': 'N7409K', 'Make': 'Piper', 'Model': 'PA 20', 'Amateur Built': 'No', 'Number of Engines': '1', 'Engine Type': '', 'FAR Description': 'Part 91: General Aviation', 'Schedule': '', 'Purpose of Flight': 'Personal', 'Air Carrier': '', 'Total Fatal Injuries': '', 'Total Serious Injuries': '', 'Total Minor Injuries': '', 'Total Uninjured': '', 'Weather Condition': 'VMC', 'Broad Phase of Flight': '', 'Report Status': 'Preliminary', 'Publication Date': '07/12/2019', '\\n': '\\n'}]\n"
     ]
    }
   ],
   "source": [
    "print(aviation_list[:2], '\\n')\n",
    "print(aviation_dict_list[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' MT': 1, ' MA': 1, ' TX': 1, ' IN': 1, ' WA': 1, ' AK': 1, ' MI': 1, ' MS': 1, ' IL': 1, ' ID': 1, ' CA': 1, ' OH': 1, ' AZ': 1, ' GA': 1, ' NY': 1, ' WV': 1, ' KS': 1, ' NE': 1, ' MN': 1, ' WY': 1, ' UT': 1, ' NC': 1, ' WI': 1, ' LA': 1, ' FL': 1, ' KY': 1, ' OR': 1, ' OK': 1, ' RI': 1, ' HI': 1, ' AL': 1, ' IA': 1, ' TN': 1, ' PA': 1, ' NJ': 1, ' MD': 1, ' VA': 1, ' MO': 1, ' NM': 1, ' NV': 1, ' CO': 1, ' ME': 1, ' AR': 1, ' CT': 1, ' SC': 1, ' ND': 1, ' AO': 1, ' MP': 1, ' SD': 1, ' NH': 1, ' GU': 1, ' VT': 1, ' DE': 1, ' PR': 1, ' MH': 1, ' GM': 1, ' VI': 1, ' PO': 1, ' DC': 1, ' ON': 1, ' Maui': 1, ' San Juan Is.': 1, ' UN': 1, ' NYC': 1, 'Kauai': 1, ' Oahu': 1, ' Kauai': 1, ' MAUI': 1, 'MAUI': 1, ' KAUAI': 1, ' OAHU': 1, 'MOLOKAI': 1, 'OAHU': 1, \" MANU'A\": 1, ' HONOLULU': 1}\n"
     ]
    }
   ],
   "source": [
    "us_list = [s for s in aviation_list if s[5] == 'United States' and s[4] != '']\n",
    "state_accidents = {}\n",
    "for line in us_list:\n",
    "    state = line[4].split(',')[1]\n",
    "    if state in dict:\n",
    "        state_accidents[state] += 1\n",
    "    else:\n",
    "        state_accidents[state] = 1\n",
    "print(state_accidents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " AK: 1\n",
      " AL: 1\n",
      " AO: 1\n",
      " AR: 1\n",
      " AZ: 1\n",
      " CA: 1\n",
      " CO: 1\n",
      " CT: 1\n",
      " DC: 1\n",
      " DE: 1\n",
      " FL: 1\n",
      " GA: 1\n",
      " GM: 1\n",
      " GU: 1\n",
      " HI: 1\n",
      " HONOLULU: 1\n",
      " IA: 1\n",
      " ID: 1\n",
      " IL: 1\n",
      " IN: 1\n",
      " KAUAI: 1\n",
      " KS: 1\n",
      " KY: 1\n",
      " Kauai: 1\n",
      " LA: 1\n",
      " MA: 1\n",
      " MANU'A: 1\n",
      " MAUI: 1\n",
      " MD: 1\n",
      " ME: 1\n",
      " MH: 1\n",
      " MI: 1\n",
      " MN: 1\n",
      " MO: 1\n",
      " MP: 1\n",
      " MS: 1\n",
      " MT: 1\n",
      " Maui: 1\n",
      " NC: 1\n",
      " ND: 1\n",
      " NE: 1\n",
      " NH: 1\n",
      " NJ: 1\n",
      " NM: 1\n",
      " NV: 1\n",
      " NY: 1\n",
      " NYC: 1\n",
      " OAHU: 1\n",
      " OH: 1\n",
      " OK: 1\n",
      " ON: 1\n",
      " OR: 1\n",
      " Oahu: 1\n",
      " PA: 1\n",
      " PO: 1\n",
      " PR: 1\n",
      " RI: 1\n",
      " SC: 1\n",
      " SD: 1\n",
      " San Juan Is.: 1\n",
      " TN: 1\n",
      " TX: 1\n",
      " UN: 1\n",
      " UT: 1\n",
      " VA: 1\n",
      " VI: 1\n",
      " VT: 1\n",
      " WA: 1\n",
      " WI: 1\n",
      " WV: 1\n",
      " WY: 1\n",
      "Kauai: 1\n",
      "MAUI: 1\n",
      "MOLOKAI: 1\n",
      "OAHU: 1\n"
     ]
    }
   ],
   "source": [
    "for key, value in sorted(state_accidents.items()):\n",
    "    print(\"%s: %s\" % (key, value))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xueho\\Projects\\Python_projects\\Raw_data\n"
     ]
    }
   ],
   "source": [
    "cd Raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Event Id', 'Investigation Type', 'Accident Number', 'Event Date',\n",
      "       'Location', 'Country', 'Latitude', 'Longitude', 'Airport Code',\n",
      "       'Airport Name', 'Injury Severity', 'Aircraft Damage',\n",
      "       'Aircraft Category', 'Registration Number', 'Make', 'Model',\n",
      "       'Amateur Built', 'Number of Engines', 'Engine Type', 'FAR Description',\n",
      "       'Schedule', 'Purpose of Flight', 'Air Carrier', 'Total Fatal Injuries',\n",
      "       'Total Serious Injuries', 'Total Minor Injuries', 'Total Uninjured',\n",
      "       'Weather Condition', 'Broad Phase of Flight', 'Report Status',\n",
      "       'Publication Date', ''],\n",
      "      dtype='object')\n",
      "(83389, 32)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "aviation_data = pd.read_csv(\"../Raw_data/AviationData.txt\", sep= \"|\", engine=\"python\")\n",
    "aviation_data.columns = aviation_data.columns.str.lstrip().str.rstrip()\n",
    "aviation_data[\"Accident Number\"]  = aviation_data[\"Accident Number\"].str.lstrip().str.rstrip()\n",
    "print(aviation_data.columns)\n",
    "print(aviation_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Event Id Investigation Type Accident Number    Event Date  \\\n",
      "83387  20001218X45447           Accident       LAX94LA336   07/19/1962    \n",
      "\n",
      "               Location          Country Latitude Longitude Airport Code  \\\n",
      "83387   BRIDGEPORT, CA    United States                                    \n",
      "\n",
      "      Airport Name  ... Air Carrier Total Fatal Injuries  \\\n",
      "83387               ...                               4    \n",
      "\n",
      "      Total Serious Injuries Total Minor Injuries Total Uninjured  \\\n",
      "83387                     0                    0               0    \n",
      "\n",
      "      Weather Condition Broad Phase of Flight     Report Status  \\\n",
      "83387              UNK               UNKNOWN    Probable Cause    \n",
      "\n",
      "      Publication Date     \n",
      "83387      09/19/1996      \n",
      "\n",
      "[1 rows x 32 columns]\n"
     ]
    }
   ],
   "source": [
    "lax_code = aviation_data[aviation_data[\"Accident Number\"] == \"LAX94LA336\"]\n",
    "print(lax_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " CA          8533\n",
      " TX          5544\n",
      " FL          5517\n",
      " AK          5394\n",
      " AZ          2692\n",
      "             ... \n",
      " ON             1\n",
      " LA             1\n",
      " MH             1\n",
      " HONOLULU       1\n",
      "MOLOKAI         1\n",
      "Name: state, Length: 77, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "aviation_data[\"Country\"]  = aviation_data[\"Country\"].str.lstrip().str.rstrip()\n",
    "us_data = aviation_data[aviation_data[\"Country\"] == \"United States\"]\n",
    "us_data[\"state\"] = us_data[\"Location\"].str.split(',').str[1]\n",
    "state_accidents = us_data[\"state\"].value_counts(ascending=False, dropna=True)\n",
    "print(state_accidents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_data[\"state\"] = us_data[\"Location\"].str.split(',').str[1]\n",
    "state_accidents = us_data[\"state\"].value_counts(ascending=False, dropna=True)\n",
    "print(state_accidents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
