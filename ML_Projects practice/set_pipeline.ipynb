{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In scikit-learn, an estimator for classification is a Python object that implements the methods fit(X, y) and predict(T).\n",
    "from sklearn import svm\n",
    "clf = svm.SVC(gamma=0.001, C=100.)\n",
    "\n",
    "# Choosing the parameters of the model: In this example, we set the value of gamma manually. To find good values for these parameters, \n",
    "# we can use tools such as grid search and cross validation.\n",
    "clf.fit(digits.data[:-1], digits.target[:-1])\n",
    "   SVC(C=100.0, gamma=0.001)\n",
    "\n",
    "clf.predict(digits.data[-1:])\n",
    "   array([8])\n",
    "    \n",
    "# save a model in scikit-learn by using Python’s built-in persistence model, pickle\n",
    "from sklearn import svm\n",
    "from sklearn import datasets\n",
    "clf = svm.SVC()\n",
    "X, y = datasets.load_iris(return_X_y=True)\n",
    "clf.fit(X, y)\n",
    "    SVC()\n",
    "\n",
    "import pickle\n",
    "s = pickle.dumps(clf)\n",
    "clf2 = pickle.loads(s)\n",
    "clf2.predict(X[0:1])\n",
    "    array([0])\n",
    "y[0]\n",
    "    0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  joblib’s replacement for pickle (joblib.dump & joblib.load), which is more efficient on big data but it can only pickle \n",
    "# to the disk and not to a string:\n",
    "from joblib import dump, load\n",
    "dump(clf, 'filename.joblib') \n",
    "clf = load('filename.joblib')\n",
    "\n",
    "# Refitting and updating parameters\n",
    "# Hyper-parameters of an estimator can be updated after it has been constructed via the set_params() method. \n",
    "# Calling fit() more than once will overwrite what was learned by any previous fit()\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.svm import SVC\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "clf = SVC()\n",
    "clf.set_params(kernel='linear').fit(X, y)\n",
    "    SVC(kernel='linear')\n",
    "clf.predict(X[:5])\n",
    "    array([0, 0, 0, 0, 0])\n",
    "\n",
    "clf.set_params(kernel='rbf').fit(X, y)\n",
    "    SVC()\n",
    "clf.predict(X[:5])\n",
    "    array([0, 0, 0, 0, 0])\n",
    "    \n",
    "# Here, the default kernel rbf is first changed to linear via SVC.set_params() after the estimator has been constructed, \n",
    "# and changed back to rbf to refit the estimator and to make a second prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn import tree\n",
    "\n",
    "# Load and split the data\n",
    "iris = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Construct some pipelines\n",
    "pipe_lr = Pipeline([('scl', StandardScaler()),\n",
    "\t\t\t('pca', PCA(n_components=2)),\n",
    "\t\t\t('clf', LogisticRegression(random_state=42))])\n",
    "\n",
    "pipe_svm = Pipeline([('scl', StandardScaler()),\n",
    "\t\t\t('pca', PCA(n_components=2)),\n",
    "\t\t\t('clf', svm.SVC(random_state=42))])\n",
    "\t\t\t\n",
    "pipe_dt = Pipeline([('scl', StandardScaler()),\n",
    "\t\t\t('pca', PCA(n_components=2)),\n",
    "\t\t\t('clf', tree.DecisionTreeClassifier(random_state=42))])\n",
    "\n",
    "# List of pipelines for ease of iteration\n",
    "pipelines = [pipe_lr, pipe_svm, pipe_dt]\n",
    "\t\t\t\n",
    "# Dictionary of pipelines and classifier types for ease of reference\n",
    "pipe_dict = {0: 'Logistic Regression', 1: 'Support Vector Machine', 2: 'Decision Tree'}\n",
    "\n",
    "# Fit the pipelines\n",
    "for pipe in pipelines:\n",
    "\tpipe.fit(X_train, y_train)\n",
    "\n",
    "# Compare accuracies\n",
    "for idx, val in enumerate(pipelines):\n",
    "\tprint('%s pipeline test accuracy: %.3f' % (pipe_dict[idx], val.score(X_test, y_test)))\n",
    "\n",
    "# Identify the most accurate model on test data\n",
    "best_acc = 0.0\n",
    "best_clf = 0\n",
    "best_pipe = ''\n",
    "for idx, val in enumerate(pipelines):\n",
    "\tif val.score(X_test, y_test) > best_acc:\n",
    "\t\tbest_acc = val.score(X_test, y_test)\n",
    "\t\tbest_pipe = val\n",
    "\t\tbest_clf = idx\n",
    "print('Classifier with best accuracy: %s' % pipe_dict[best_clf])\n",
    "\n",
    "# Save pipeline to file\n",
    "joblib.dump(best_pipe, 'best_pipeline.pkl', compress=1)\n",
    "print('Saved %s pipeline to file' % pipe_dict[best_clf])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
